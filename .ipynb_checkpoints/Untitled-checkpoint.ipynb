{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9bcfca-a948-4742-b96f-d4a009d73136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Healthcare User Story Extractor (Vertex AI + RAG + Async + BigQuery Export)\n",
    "# ---------------------------------------------------------------------------\n",
    "# Parses healthcare requirement docs and extracts validated user stories\n",
    "# using Google Vertex AI (Gemini + Embeddings). Optionally exports results\n",
    "# to BigQuery with dynamic schema inference.\n",
    "\n",
    "# Deps:\n",
    "#   pip install google-cloud-aiplatform langchain-google-vertexai PyPDF2 python-docx pydantic numpy tqdm\n",
    "# Auth:\n",
    "#   gcloud auth application-default login\n",
    "# \"\"\"\n",
    "\n",
    "# LLM_MODEL = \"gemini-2.0-flash\"  # e.g., \"gemini-1.5-pro\"\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# import uuid\n",
    "# import asyncio\n",
    "# import docx\n",
    "# import xml.etree.ElementTree as ET\n",
    "# import numpy as np\n",
    "# from tqdm.auto import tqdm\n",
    "# from PyPDF2 import PdfReader\n",
    "# from typing import List, Iterable, Dict, Any, Optional\n",
    "# from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# from google.cloud import bigquery\n",
    "# from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
    "\n",
    "# # ========================== Helpers & Schema ==========================\n",
    "\n",
    "# def _story_text_for_embedding(s: dict) -> str:\n",
    "#     \"\"\"Flatten story + ACs for richer similarity signals.\"\"\"\n",
    "#     ac = s.get(\"acceptance_criteria\", []) or []\n",
    "#     ac_text = \" | \".join([f\"G:{a.get('given','')} W:{a.get('when','')} T:{a.get('then','')}\" for a in ac])\n",
    "#     return f\"{s.get('user_story','')} || {ac_text}\"\n",
    "\n",
    "# class AcceptanceCriteria(BaseModel):\n",
    "#     given: str\n",
    "#     when: str\n",
    "#     then: str\n",
    "\n",
    "# class Citation(BaseModel):\n",
    "#     page: int\n",
    "#     snippet: str\n",
    "\n",
    "# class UserStory(BaseModel):\n",
    "#     epic: str\n",
    "#     story_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n",
    "#     user_story: str\n",
    "#     acceptance_criteria: List[AcceptanceCriteria]\n",
    "#     priority: str\n",
    "#     dependencies: List[str] = []\n",
    "#     non_functional: List[str] = []\n",
    "#     source_requirement_ids: List[str] = []\n",
    "#     assumptions: List[str] = []\n",
    "#     open_questions: List[str] = []\n",
    "#     citations: List[Citation] = []   # NEW\n",
    "\n",
    "# SCHEMA = {\n",
    "#     \"epic\": \"\",\n",
    "#     \"story_id\": \"\",\n",
    "#     \"user_story\": \"As a <role>, I want <capability> so that <benefit>.\",\n",
    "#     \"acceptance_criteria\": [\n",
    "#         {\"given\": \"\", \"when\": \"\", \"then\": \"\"}\n",
    "#     ],\n",
    "#     \"priority\": \"Must|Should|Could|Won't\",\n",
    "#     \"dependencies\": [],\n",
    "#     \"non_functional\": [],\n",
    "#     \"source_requirement_ids\": [],\n",
    "#     \"assumptions\": [],\n",
    "#     \"open_questions\": [],\n",
    "#     \"citations\": [{\"page\": 0, \"snippet\": \"\"}]  # NEW\n",
    "# }\n",
    "\n",
    "# # ========================== Parsing & Normalization ==========================\n",
    "\n",
    "# BULLET_RE = re.compile(r\"^\\s*(?:[-*•]|[0-9]+[.)])\\s+\")\n",
    "# REQ_ID_RE  = re.compile(r\"^(REQ[-\\s]?\\d+|[A-Z]{2,}\\d+|[0-9]+(?:\\.[0-9]+)*)\\b\")\n",
    "# REQ_KEYWORDS = re.compile(r\"\\b(system shall|shall|must|should|ability to|capability to|enable|allow)\\b\", re.I)\n",
    "\n",
    "# def parse_pdf_pages(path: str) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"Return list of {'page': int, 'text': str} for PDFs.\"\"\"\n",
    "#     reader = PdfReader(path)\n",
    "#     pages = []\n",
    "#     for idx, page in enumerate(reader.pages, start=1):\n",
    "#         pages.append({\"page\": idx, \"text\": page.extract_text() or \"\"})\n",
    "#     return pages\n",
    "\n",
    "# def parse_docx(path: str) -> str:\n",
    "#     doc = docx.Document(path)\n",
    "#     return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
    "\n",
    "# def parse_xml(path: str) -> str:\n",
    "#     tree = ET.parse(path)\n",
    "#     root = tree.getroot()\n",
    "#     return ET.tostring(root, encoding=\"unicode\")\n",
    "\n",
    "# def parse_json_text(path: str) -> str:\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return json.dumps(json.load(f), indent=2, ensure_ascii=False)\n",
    "\n",
    "# def parse_file_text_or_pages(path: str) -> Dict[str, Any]:\n",
    "#     \"\"\"Return {'text': str} for non-PDFs, or {'pages': [{page, text}, ...]} for PDFs.\"\"\"\n",
    "#     path_l = path.lower()\n",
    "#     if path_l.endswith(\".pdf\"):\n",
    "#         return {\"pages\": parse_pdf_pages(path)}\n",
    "#     if path_l.endswith(\".docx\"):\n",
    "#         return {\"text\": parse_docx(path)}\n",
    "#     if path_l.endswith(\".xml\"):\n",
    "#         return {\"text\": parse_xml(path)}\n",
    "#     if path_l.endswith(\".json\"):\n",
    "#         return {\"text\": parse_json_text(path)}\n",
    "#     with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "#         return {\"text\": f.read()}\n",
    "\n",
    "# def normalize_text(text: str) -> str:\n",
    "#     \"\"\"Join soft-wrapped lines, preserve bullets/headings, keep paragraph breaks.\"\"\"\n",
    "#     lines = text.splitlines()\n",
    "#     out = []\n",
    "#     buf = \"\"\n",
    "#     for raw in lines:\n",
    "#         line = raw.rstrip()\n",
    "#         # blank line => paragraph break\n",
    "#         if not line.strip():\n",
    "#             if buf:\n",
    "#                 out.append(buf)\n",
    "#                 buf = \"\"\n",
    "#             out.append(\"\")\n",
    "#             continue\n",
    "#         # new bullet or heading/ID => start new requirement block\n",
    "#         if BULLET_RE.match(line) or REQ_ID_RE.match(line):\n",
    "#             if buf:\n",
    "#                 out.append(buf)\n",
    "#             buf = line.strip()\n",
    "#             continue\n",
    "#         # soft wrap: if buf doesn't end sentence punctuation, join\n",
    "#         if buf and not buf.endswith((\".\", \":\", \";\")):\n",
    "#             buf += \" \" + line.strip()\n",
    "#         else:\n",
    "#             if buf:\n",
    "#                 out.append(buf)\n",
    "#             buf = line.strip()\n",
    "#     if buf:\n",
    "#         out.append(buf)\n",
    "#     # collapse consecutive blanks\n",
    "#     cleaned = []\n",
    "#     for s in out:\n",
    "#         if s == \"\" and cleaned and cleaned[-1] == \"\":\n",
    "#             continue\n",
    "#         cleaned.append(s)\n",
    "#     return \"\\n\".join(cleaned)\n",
    "\n",
    "# def normalize_page_text(pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "#     return [{\"page\": p[\"page\"], \"text\": normalize_text(p[\"text\"])} for p in pages]\n",
    "\n",
    "# import re\n",
    "# from typing import List, Dict\n",
    "\n",
    "# # ========================== Heading-based Epic Extraction ==========================\n",
    "\n",
    "# # ========================== Heading-based Epic Extraction ==========================\n",
    "\n",
    "# HEADING_RE = re.compile(r\"^(\\d+(?:\\.\\d+)*)(?:\\s+)([A-Z][\\w\\s-]+.*)$\")\n",
    "\n",
    "# # Generic section titles we don't want to use as epics\n",
    "# GENERIC_HEADINGS = {\n",
    "#     \"introduction\", \"purpose\", \"scope\",\n",
    "#     \"functional requirements\", \"non-functional requirements\",\n",
    "#     \"references\", \"appendix\"\n",
    "# }\n",
    "\n",
    "# def extract_headings(text: str) -> Dict[str, str]:\n",
    "#     \"\"\"\n",
    "#     Extract headings like '2.1.2 Clinician Portal Development'\n",
    "#     Returns dict: {'2.1.2': 'Clinician Portal Development'}\n",
    "#     Skips generic titles.\n",
    "#     \"\"\"\n",
    "#     headings = {}\n",
    "#     for line in text.splitlines():\n",
    "#         m = HEADING_RE.match(line.strip())\n",
    "#         if m:\n",
    "#             sec_id = m.group(1)\n",
    "#             title = m.group(2).strip()\n",
    "#             # Skip generic container titles\n",
    "#             if title.lower() not in GENERIC_HEADINGS:\n",
    "#                 headings[sec_id] = title\n",
    "#     return headings\n",
    "\n",
    "# def assign_epic(req_id: str, headings: Dict[str, str]) -> str:\n",
    "#     \"\"\"\n",
    "#     Assign epic based on the closest heading prefix.\n",
    "#     Example: req_id='2.1.2.5' → epic='Clinician Portal Development'\n",
    "#     Falls back to 'General' if none found.\n",
    "#     \"\"\"\n",
    "#     for h in sorted(headings.keys(), key=lambda x: -len(x)):  # longest prefix first\n",
    "#         if req_id.startswith(h):\n",
    "#             return headings[h]\n",
    "#     return \"General\"\n",
    "\n",
    "\n",
    "\n",
    "# def split_requirements_with_epics(text: str) -> List[Dict[str, str]]:\n",
    "#     \"\"\"\n",
    "#     Segment into atomic requirements with IDs (AUTO-n fallback),\n",
    "#     and attach inferred epics from document headings.\n",
    "#     \"\"\"\n",
    "#     headings = extract_headings(text)\n",
    "#     lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "#     requirements = []\n",
    "#     cur = None\n",
    "\n",
    "#     def flush():\n",
    "#         nonlocal cur\n",
    "#         if cur and cur[\"text\"].strip():\n",
    "#             cur[\"text\"] = re.sub(r\"\\s+\", \" \", cur[\"text\"]).strip()\n",
    "#             # Assign epic from headings\n",
    "#             cur[\"epic\"] = assign_epic(cur[\"req_id\"], headings)\n",
    "#             requirements.append(cur)\n",
    "#         cur = None\n",
    "\n",
    "#     for i, line in enumerate(lines):\n",
    "#         rid = None\n",
    "#         content = line\n",
    "#         start_new = False\n",
    "\n",
    "#         # Requirement ID pattern\n",
    "#         m = re.match(r\"^(REQ[-\\s]?\\d+|[0-9]+(?:\\.[0-9]+)*)\\b\", line)\n",
    "#         if m:\n",
    "#             rid = m.group(0)\n",
    "#             start_new = True\n",
    "#             content = line[m.end():].strip() or line.strip()\n",
    "#         elif re.match(r\"^\\s*[-*•]\\s+\", line):  # bullet point\n",
    "#             start_new = True\n",
    "#             content = line.strip()\n",
    "\n",
    "#         if start_new:\n",
    "#             flush()\n",
    "#             rid = rid or f\"AUTO-{len(requirements)+1}\"\n",
    "#             cur = {\"req_id\": rid, \"text\": content}\n",
    "#         else:\n",
    "#             if cur:\n",
    "#                 cur[\"text\"] += \" \" + content\n",
    "#             else:\n",
    "#                 cur = {\"req_id\": f\"AUTO-{len(requirements)+1}\", \"text\": content}\n",
    "\n",
    "#     flush()\n",
    "#     return requirements\n",
    "\n",
    "\n",
    "# def chunked(seq: List[Any], n: int) -> Iterable[List[Any]]:\n",
    "#     for i in range(0, len(seq), n):\n",
    "#         yield seq[i:i+n]\n",
    "\n",
    "# # ========================== RAG Index & Retrieval ==========================\n",
    "\n",
    "# def page_chunks(pages: List[Dict[str, Any]], max_chars=1500, overlap=200) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"Create sliding-window chunks with page provenance.\"\"\"\n",
    "#     chunks = []\n",
    "#     for p in pages:\n",
    "#         t = p[\"text\"]\n",
    "#         i = 0\n",
    "#         while i < len(t):\n",
    "#             j = min(len(t), i + max_chars)\n",
    "#             chunk = t[i:j]\n",
    "#             chunks.append({\"page\": p[\"page\"], \"text\": chunk})\n",
    "#             if j == len(t): break\n",
    "#             i = j - overlap\n",
    "#     return chunks\n",
    "\n",
    "# def _cosines(query_emb, matrix):\n",
    "#     sims = []\n",
    "#     q = np.array(query_emb, dtype=float)\n",
    "#     qn = np.linalg.norm(q)\n",
    "#     for v in matrix:\n",
    "#         v = np.array(v, dtype=float)\n",
    "#         vn = np.linalg.norm(v)\n",
    "#         sims.append(0.0 if qn == 0 or vn == 0 else float(np.dot(q, v) / (qn * vn)))\n",
    "#     return sims\n",
    "\n",
    "# def build_retriever(embedder: VertexAIEmbeddings, chunks: List[Dict[str, Any]]):\n",
    "#     texts = [c[\"text\"] for c in chunks]\n",
    "#     embs = embedder.embed_documents(texts)\n",
    "#     return {\"chunks\": chunks, \"embs\": embs}\n",
    "\n",
    "# def retrieve_context(retriever: Dict[str, Any], embedder: VertexAIEmbeddings, query: str, top_k=3):\n",
    "#     q_emb = embedder.embed_query(query)\n",
    "#     sims = _cosines(q_emb, retriever[\"embs\"])\n",
    "#     idxs = np.argsort(sims)[::-1][:top_k]\n",
    "#     results = []\n",
    "#     for i in idxs:\n",
    "#         c = retriever[\"chunks\"][i]\n",
    "#         results.append({\"page\": c[\"page\"], \"text\": c[\"text\"], \"score\": float(sims[i])})\n",
    "#     return results\n",
    "\n",
    "# # ========================== LLM Utils ==========================\n",
    "\n",
    "# async def safe_llm_batch_async(llm, prompts, timeout=60):\n",
    "#     \"\"\"Run multiple LLM calls concurrently with timeout handling.\"\"\"\n",
    "#     tasks = [llm.ainvoke(p) for p in prompts]  # VertexAI async call\n",
    "#     try:\n",
    "#         return await asyncio.wait_for(asyncio.gather(*tasks, return_exceptions=True), timeout=timeout)\n",
    "#     except asyncio.TimeoutError:\n",
    "#         print(\"⏳ Timeout reached, skipping batch.\")\n",
    "#         return [\"{}\" for _ in prompts]\n",
    "\n",
    "# FENCE_OPEN_RE = re.compile(r\"^```(?:json|JSON)?\\s*\")\n",
    "# FENCE_CLOSE_RE = re.compile(r\"\\s*```$\")\n",
    "\n",
    "# def clean_response(resp) -> str:\n",
    "#     \"\"\"Convert VertexAI response to a plain JSON string. Strips ```json fences.\"\"\"\n",
    "#     if isinstance(resp, Exception):\n",
    "#         return \"{}\"\n",
    "#     if isinstance(resp, str):\n",
    "#         text = resp.strip()\n",
    "#     elif hasattr(resp, \"content\") and isinstance(resp.content, str):\n",
    "#         text = resp.content.strip()\n",
    "#     elif hasattr(resp, \"content\"):\n",
    "#         text = str(resp.content).strip()\n",
    "#     else:\n",
    "#         text = str(resp).strip()\n",
    "\n",
    "#     # Strip code fences\n",
    "#     text = FENCE_OPEN_RE.sub(\"\", text)\n",
    "#     text = FENCE_CLOSE_RE.sub(\"\", text)\n",
    "#     if \"```\" in text:\n",
    "#         text = text.replace(\"```json\", \"\").replace(\"```JSON\", \"\").replace(\"```\", \"\")\n",
    "#     return text.strip()\n",
    "\n",
    "# def extract_json_object(text: str) -> str:\n",
    "#     \"\"\"Try to salvage the first balanced {...} block from text.\"\"\"\n",
    "#     start = text.find(\"{\")\n",
    "#     if start == -1:\n",
    "#         return text\n",
    "#     depth = 0\n",
    "#     for i in range(start, len(text)):\n",
    "#         ch = text[i]\n",
    "#         if ch == \"{\":\n",
    "#             depth += 1\n",
    "#         elif ch == \"}\":\n",
    "#             depth -= 1\n",
    "#             if depth == 0:\n",
    "#                 return text[start:i+1]\n",
    "#     return text[start:]  # fallback\n",
    "\n",
    "# def cosine_similarity(a, b) -> float:\n",
    "#     a = np.array(a, dtype=float)\n",
    "#     b = np.array(b, dtype=float)\n",
    "#     denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "#     if denom == 0.0:\n",
    "#         return 0.0\n",
    "#     return float(np.dot(a, b) / denom)\n",
    "\n",
    "# # ========================== Alignment Checks ==========================\n",
    "\n",
    "# STOP = set((\"the\",\"and\",\"of\",\"to\",\"in\",\"a\",\"for\",\"on\",\"with\",\"by\",\"is\",\"be\",\"as\",\"at\",\"or\",\"an\",\"from\"))\n",
    "\n",
    "# def key_terms(s: str):\n",
    "#     toks = re.findall(r\"[A-Za-z0-9_]+\", s.lower())\n",
    "#     return set(t for t in toks if t not in STOP and len(t) > 2)\n",
    "\n",
    "# def alignment_score(req_text: str, citations: List[dict]) -> float:\n",
    "#     req_terms = key_terms(req_text)\n",
    "#     ctx_terms = set()\n",
    "#     for c in citations or []:\n",
    "#         ctx_terms |= key_terms(c.get(\"snippet\",\"\"))\n",
    "#     if not req_terms or not ctx_terms:\n",
    "#         return 0.0\n",
    "#     return len(req_terms & ctx_terms) / float(len(req_terms | ctx_terms))\n",
    "\n",
    "# def validate_alignment(requirement: Dict[str, str], story: Dict[str, Any], min_score=0.15):\n",
    "#     score = alignment_score(requirement.get(\"text\",\"\"), story.get(\"citations\", []))\n",
    "#     ok = score >= min_score and len(story.get(\"citations\", [])) > 0\n",
    "#     return ok, score\n",
    "# import csv\n",
    "\n",
    "# import csv\n",
    "\n",
    "# def export_testcases_csv(\n",
    "#     stories,\n",
    "#     requirements,\n",
    "#     out_csv=\"testcases.csv\",\n",
    "#     area_path=\"Healthcare\\\\DayHealth\",\n",
    "#     iteration_path=\"Release 1\",\n",
    "#     synth_step_on_missing_ac=True\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     ADO-style export.\n",
    "#     Writes one row per acceptance criterion.\n",
    "#     If a story has no ACs and synth_step_on_missing_ac=True,\n",
    "#     writes a single synthetic step so nothing is dropped.\n",
    "#     \"\"\"\n",
    "\n",
    "#     if not stories:\n",
    "#         print(\"⚠️ No stories to export.\")\n",
    "#         return\n",
    "\n",
    "#     req_map = {r[\"req_id\"]: r for r in requirements}\n",
    "\n",
    "#     total_stories = len(stories)\n",
    "#     stories_with_acs = 0\n",
    "#     stories_without_acs = 0\n",
    "#     rows_written = 0\n",
    "\n",
    "#     with open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "#         w = csv.writer(f)\n",
    "#         w.writerow([\n",
    "#             \"Test Case Title\",\n",
    "#             \"Step Action\",\n",
    "#             \"Step Expected\",\n",
    "#             \"Requirement ID\",\n",
    "#             \"Priority\",\n",
    "#             \"Tags\",\n",
    "#             \"Pages\",\n",
    "#             \"Story Id\",\n",
    "#             \"Epic\",\n",
    "#             \"Area Path\",\n",
    "#             \"Iteration Path\",\n",
    "#         ])\n",
    "\n",
    "#         for s in stories:\n",
    "#             rid = (s.get(\"source_requirement_ids\") or [None])[0]\n",
    "#             req = req_map.get(rid, {})\n",
    "#             epic = (s.get(\"epic\") or req.get(\"epic\") or \"General\").strip()\n",
    "\n",
    "#             # build tags\n",
    "#             priority = (s.get(\"priority\") or \"\").strip()\n",
    "#             tags = []\n",
    "#             if priority:\n",
    "#                 tags.append(f\"@priority_{priority}\")\n",
    "#             if rid:\n",
    "#                 tags.append(f\"@req_{rid}\")\n",
    "#             nf_join = \" \".join(s.get(\"non_functional\", []) or [])\n",
    "#             if \"HIPAA\" in nf_join.upper():\n",
    "#                 tags.append(\"@HIPAA\")\n",
    "#             if \"21 CFR\" in nf_join.upper() or \"FDA\" in nf_join.upper():\n",
    "#                 tags.append(\"@FDA21CFR11\")\n",
    "#             tags_str = \" \".join(tags)\n",
    "\n",
    "#             # pages from citations\n",
    "#             pages = \";\".join(\n",
    "#                 [str(c.get(\"page\")) for c in (s.get(\"citations\") or []) if c.get(\"page\")]\n",
    "#             )\n",
    "\n",
    "#             acs = s.get(\"acceptance_criteria\") or []\n",
    "#             if acs:\n",
    "#                 stories_with_acs += 1\n",
    "#                 for ac in acs:\n",
    "#                     given_ = (ac.get(\"given\") or \"\").strip()\n",
    "#                     when_  = (ac.get(\"when\") or \"\").strip()\n",
    "#                     then_  = (ac.get(\"then\") or \"\").strip()\n",
    "\n",
    "#                     step_action = \" | \".join([p for p in [given_, when_] if p])\n",
    "#                     step_expected = then_ or \"Then: expected outcome is observed.\"\n",
    "\n",
    "#                     w.writerow([\n",
    "#                         s.get(\"user_story\",\"\").strip(),\n",
    "#                         step_action,\n",
    "#                         step_expected,\n",
    "#                         rid,\n",
    "#                         priority,\n",
    "#                         tags_str,\n",
    "#                         pages,\n",
    "#                         s.get(\"story_id\",\"\"),\n",
    "#                         epic,\n",
    "#                         area_path,\n",
    "#                         iteration_path,\n",
    "#                     ])\n",
    "#                     rows_written += 1\n",
    "#             else:\n",
    "#                 stories_without_acs += 1\n",
    "#                 if synth_step_on_missing_ac:\n",
    "#                     # create a single synthetic step so the test case isn’t lost\n",
    "#                     title = s.get(\"user_story\",\"\").strip() or \"User story (no AC specified)\"\n",
    "#                     w.writerow([\n",
    "#                         title,\n",
    "#                         \"Given the system is available | When I perform the described capability\",\n",
    "#                         \"Then the described outcome is achieved\",\n",
    "#                         rid,\n",
    "#                         priority,\n",
    "#                         tags_str,\n",
    "#                         pages,\n",
    "#                         s.get(\"story_id\",\"\"),\n",
    "#                         epic,\n",
    "#                         area_path,\n",
    "#                         iteration_path,\n",
    "#                     ])\n",
    "#                     rows_written += 1\n",
    "#                 # else: intentionally skip writing rows for no-AC stories\n",
    "\n",
    "#     print(\"✅ Exported test cases to\", out_csv)\n",
    "#     print(f\"   Stories total:            {total_stories}\")\n",
    "#     print(f\"   Stories with ACs:         {stories_with_acs}\")\n",
    "#     print(f\"   Stories without ACs:      {stories_without_acs}\")\n",
    "#     print(f\"   Rows written (test steps):{rows_written}\")\n",
    "\n",
    "\n",
    "# # ========================== Extractor ==========================\n",
    "\n",
    "# class HealthcareStoryExtractor:\n",
    "#     def __init__(self, project_id, location=\"us-central1\",\n",
    "#                  embedding_model=\"text-embedding-005\",\n",
    "#                  classifier_model=LLM_MODEL):\n",
    "#         self.project_id = project_id\n",
    "#         self.location = location\n",
    "#         self.embedder = VertexAIEmbeddings(\n",
    "#             model=embedding_model,\n",
    "#             project=project_id,\n",
    "#             location=location\n",
    "#         )\n",
    "#         self.llm = VertexAI(\n",
    "#             model_name=classifier_model,\n",
    "#             temperature=0.2,   # slight diversity, still stable JSON\n",
    "#             top_p=0.9,\n",
    "#             top_k=40,\n",
    "#             project=project_id,\n",
    "#             location=location,\n",
    "#         )\n",
    "\n",
    "#     async def generate_user_stories_batch(\n",
    "#         self,\n",
    "#         requirements,\n",
    "#         glossary,\n",
    "#         actors,\n",
    "#         constraints,\n",
    "#         batch_size=5,\n",
    "#         retriever: Optional[Dict[str, Any]] = None,\n",
    "#     ):\n",
    "#         # Use dumps to avoid quote-escaping hell in long strings\n",
    "#         abstain = {\n",
    "#             \"epic\": \"\",\n",
    "#             \"story_id\": \"\",\n",
    "#             \"user_story\": \"\",\n",
    "#             \"acceptance_criteria\": [],\n",
    "#             \"priority\": \"\",\n",
    "#             \"dependencies\": [],\n",
    "#             \"non_functional\": [],\n",
    "#             \"source_requirement_ids\": [],\n",
    "#             \"assumptions\": [\"Insufficient context\"],\n",
    "#             \"open_questions\": [\"Need clarification\"],\n",
    "#             \"citations\": [],\n",
    "#         }\n",
    "\n",
    "#         system_prompt = (\n",
    "#             \"You are a senior BA in healthcare software.\\n\"\n",
    "#             \"Use ONLY the provided glossary, actors, constraints, and CONTEXT SNIPPETS.\\n\"\n",
    "#             \"Cite which page(s) you used in the 'citations' field; include a short snippet from each page.\\n\"\n",
    "#             \"If the context is insufficient or unrelated, return EXACTLY this JSON:\\n\"\n",
    "#             f\"{json.dumps(abstain, indent=2)}\\n\"\n",
    "#             \"Return ONLY valid JSON with this schema (no markdown, no commentary):\\n\"\n",
    "#             f\"{json.dumps(SCHEMA, indent=2)}\"\n",
    "#         )\n",
    "\n",
    "#         stories = []\n",
    "#         for i in tqdm(range(0, len(requirements), batch_size), desc=\"LLM batches\"):\n",
    "#             batch_reqs = requirements[i:i + batch_size]\n",
    "#             prompts = []\n",
    "#             for req in batch_reqs:\n",
    "#                 ctx = []\n",
    "#                 if retriever is not None:\n",
    "#                     hits = retrieve_context(retriever, self.embedder, req[\"text\"], top_k=3)\n",
    "#                     ctx = [{\"page\": h[\"page\"], \"snippet\": h[\"text\"][:500]} for h in hits]  # cap snippet\n",
    "\n",
    "#                 user_prompt = (\n",
    "#                     f\"GLOSSARY: {glossary}\\n\"\n",
    "#                     f\"ACTORS: {actors}\\n\"\n",
    "#                     f\"CONSTRAINTS: {constraints}\\n\"\n",
    "#                     f\"CONTEXT SNIPPETS: {json.dumps(ctx, ensure_ascii=False)}\\n\"\n",
    "#                     f\"REQUIREMENT (ID: {req['req_id']}): {req['text']}\"\n",
    "#                 )\n",
    "#                 prompts.append(f\"{system_prompt}\\n\\n{user_prompt}\")\n",
    "\n",
    "#             responses = await safe_llm_batch_async(self.llm, prompts)\n",
    "\n",
    "#             for req, resp in zip(batch_reqs, responses):\n",
    "#                 raw_text = clean_response(resp)\n",
    "#                 try:\n",
    "#                     try:\n",
    "#                         story_json = json.loads(raw_text)\n",
    "#                     except json.JSONDecodeError:\n",
    "#                         story_json = json.loads(extract_json_object(raw_text))\n",
    "\n",
    "#                     us = UserStory(**story_json)\n",
    "\n",
    "#                     # Force unique story_id & keep provenance\n",
    "#                     us.story_id = str(uuid.uuid4())\n",
    "#                     if not us.source_requirement_ids:\n",
    "#                         us.source_requirement_ids = [req[\"req_id\"]]\n",
    "\n",
    "#                     stories.append(us.model_dump())  # pydantic v2\n",
    "#                 except (json.JSONDecodeError, ValidationError) as e:\n",
    "#                     print(f\"❌ Invalid JSON for {req['req_id']}: {e}\\nRaw output:\\n{raw_text}\\n\")\n",
    "\n",
    "#         self._last_requirements = requirements\n",
    "#         return stories\n",
    "\n",
    "\n",
    "\n",
    "#     def check_duplicates(self, stories, threshold=0.99):\n",
    "#         \"\"\"Return list of (story_id_i, story_id_j, similarity) for near-duplicates across different source reqs.\"\"\"\n",
    "#         texts = [_story_text_for_embedding(s) for s in stories if s.get(\"user_story\")]\n",
    "#         if not texts:\n",
    "#             return []\n",
    "\n",
    "#         embeddings = self.embedder.embed_documents(texts)\n",
    "#         flagged = []\n",
    "\n",
    "#         total_pairs = (len(embeddings) * (len(embeddings) - 1)) // 2\n",
    "#         pbar = tqdm(total=total_pairs, desc=\"Duplicate check\")\n",
    "#         for i in range(len(embeddings)):\n",
    "#             for j in range(i + 1, len(embeddings)):\n",
    "#                 # provenance-aware: skip same-source pairs\n",
    "#                 src_i = set(stories[i].get(\"source_requirement_ids\", []))\n",
    "#                 src_j = set(stories[j].get(\"source_requirement_ids\", []))\n",
    "#                 if not (src_i & src_j):\n",
    "#                     sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "#                     if sim >= threshold:\n",
    "#                         flagged.append((stories[i][\"story_id\"], stories[j][\"story_id\"], sim))\n",
    "#                 pbar.update(1)\n",
    "#         pbar.close()\n",
    "#         return flagged\n",
    "\n",
    "#     def dedupe_stories(self, stories, threshold=0.99):\n",
    "#         \"\"\"Cluster near-duplicates and keep the best representative per cluster.\"\"\"\n",
    "#         if not stories:\n",
    "#             return stories\n",
    "\n",
    "#         texts = [_story_text_for_embedding(s) for s in stories]\n",
    "#         embeddings = self.embedder.embed_documents(texts)\n",
    "\n",
    "#         # Union-Find with progress\n",
    "#         parent = list(range(len(stories)))\n",
    "#         def find(x):\n",
    "#             while parent[x] != x:\n",
    "#                 parent[x] = parent[parent[x]]\n",
    "#                 x = parent[x]\n",
    "#             return x\n",
    "#         def union(a, b):\n",
    "#             ra, rb = find(a), find(b)\n",
    "#             if ra != rb:\n",
    "#                 parent[rb] = ra\n",
    "\n",
    "#         total_pairs = (len(embeddings) * (len(embeddings) - 1)) // 2\n",
    "#         pbar = tqdm(total=total_pairs, desc=\"Clustering dupes\")\n",
    "#         for i in range(len(embeddings)):\n",
    "#             for j in range(i + 1, len(embeddings)):\n",
    "#                 src_i = set(stories[i].get(\"source_requirement_ids\", []))\n",
    "#                 src_j = set(stories[j].get(\"source_requirement_ids\", []))\n",
    "#                 if not (src_i & src_j):\n",
    "#                     sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "#                     if sim >= threshold:\n",
    "#                         union(i, j)\n",
    "#                 pbar.update(1)\n",
    "#         pbar.close()\n",
    "\n",
    "#         clusters = {}\n",
    "#         for idx in range(len(stories)):\n",
    "#             r = find(idx)\n",
    "#             clusters.setdefault(r, []).append(idx)\n",
    "\n",
    "#         # pick representative: more ACs, then longer text\n",
    "#         def score(k):\n",
    "#             ac_len = len(stories[k].get(\"acceptance_criteria\", []) or [])\n",
    "#             txt_len = len(texts[k])\n",
    "#             return (ac_len, txt_len)\n",
    "\n",
    "#         kept = []\n",
    "#         dropped_pairs = []  # (kept_id, dropped_id, cluster_size)\n",
    "#         for _, idxs in clusters.items():\n",
    "#             if len(idxs) == 1:\n",
    "#                 kept.append(stories[idxs[0]])\n",
    "#                 continue\n",
    "#             best = max(idxs, key=score)\n",
    "#             kept.append(stories[best])\n",
    "#             for other in idxs:\n",
    "#                 if other != best:\n",
    "#                     dropped_pairs.append((stories[best][\"story_id\"], stories[other][\"story_id\"], len(idxs)))\n",
    "\n",
    "#         if dropped_pairs:\n",
    "#             preview = [(k, d, int(n)) for k, d, n in dropped_pairs]\n",
    "#             print(\"🧹 Dedupe kept/dropped:\", preview)\n",
    "\n",
    "#         return kept\n",
    "\n",
    "\n",
    "#     async def extract_from_file(\n",
    "#         self,\n",
    "#         file_path,\n",
    "#         constraints=\"HIPAA, FDA 21 CFR Part 11\",\n",
    "#         batch_llm_size=20,\n",
    "#         llm_inner_batch=5,\n",
    "#         dedupe=True,\n",
    "#         dup_threshold=0.99,\n",
    "#         min_alignment=0.15,\n",
    "#     ):\n",
    "#         print(\"📥 Parsing document...\")\n",
    "#         parsed = parse_file_text_or_pages(file_path)\n",
    "\n",
    "#         retriever = None\n",
    "#         if \"pages\" in parsed:\n",
    "#             print(\"🧹 Normalizing PDF pages & building RAG index...\")\n",
    "#             norm_pages = normalize_page_text(parsed[\"pages\"])\n",
    "#             chunks = page_chunks(norm_pages, max_chars=1500, overlap=200)\n",
    "#             retriever = build_retriever(self.embedder, chunks)\n",
    "#             full_text = \"\\n\".join([p[\"text\"] for p in norm_pages])\n",
    "#         else:\n",
    "#             full_text = parsed[\"text\"]\n",
    "\n",
    "#         print(\"🧩 Segmenting requirements...\")\n",
    "#         requirements = split_requirements_with_epics(full_text)\n",
    "#         print(f\"📌 Found requirements: {len(requirements)}\")\n",
    "\n",
    "#         # ✅ Limit for test mode\n",
    "#         if TEST:\n",
    "#             requirements = requirements[:10]\n",
    "#             print(f\"⚡ TEST mode active → using only {len(requirements)} requirements\")\n",
    "\n",
    "#         glossary = {\"EHR\": \"Electronic Health Record\", \"HL7\": \"Data exchange standard\"}\n",
    "#         actors = {\"Doctor\": \"Reviews patient data\", \"Nurse\": \"Updates vitals\", \"Patient\": \"Views reports\"}\n",
    "\n",
    "#         stories: List[Dict[str, Any]] = []\n",
    "#         print(\"🧠 Generating stories (LLM)...\")\n",
    "#         for batch in tqdm(list(chunked(requirements, batch_llm_size)), desc=\"Requirement chunks\"):\n",
    "#             part = await self.generate_user_stories_batch(\n",
    "#                 batch, glossary, actors, constraints, batch_size=llm_inner_batch, retriever=retriever\n",
    "#             )\n",
    "#             stories.extend(part)\n",
    "\n",
    "#         print(f\"🧾 Generated stories (pre-alignment, pre-dedupe): {len(stories)}\")\n",
    "\n",
    "#         # Alignment pass\n",
    "#         print(\"🔎 Checking alignment with citations...\")\n",
    "#         req_map = {r[\"req_id\"]: r for r in requirements}\n",
    "#         aligned, needs_review = [], []\n",
    "#         for s in stories:\n",
    "#             rid = (s.get(\"source_requirement_ids\") or [None])[0]\n",
    "#             req = req_map.get(rid, {\"text\": \"\"})\n",
    "#             ok, score = validate_alignment(req, s, min_score=min_alignment)\n",
    "#             s[\"alignment_score\"] = round(score, 3)\n",
    "#             s[\"needs_review\"] = not ok\n",
    "#             (aligned if ok else needs_review).append(s)\n",
    "#         print(f\"✅ Aligned: {len(aligned)} | 🚩 Needs review: {len(needs_review)}\")\n",
    "\n",
    "#         final_stories = aligned + needs_review\n",
    "#         if dedupe and final_stories:\n",
    "#             dups = self.check_duplicates(final_stories, threshold=dup_threshold)\n",
    "#             if dups:\n",
    "#                 print(\"⚠️ Near-duplicate pairs:\", [(a, b, round(sim, 3)) for a, b, sim in dups])\n",
    "#             final_stories = self.dedupe_stories(final_stories, threshold=dup_threshold)\n",
    "#             print(f\"✅ Final stories after dedupe: {len(final_stories)}\")\n",
    "#         else:\n",
    "#             print(\"⏭️ Skipping dedupe.\")\n",
    "\n",
    "#         return final_stories\n",
    "\n",
    "\n",
    "\n",
    "#     # ------------------ BigQuery Export ------------------\n",
    "\n",
    "#     def _infer_bq_schema(self, sample_row: dict):\n",
    "#         \"\"\"Infer BigQuery schema dynamically from a sample story dict.\"\"\"\n",
    "#         schema = []\n",
    "#         for key, val in sample_row.items():\n",
    "#             if isinstance(val, list) and val and isinstance(val[0], dict):\n",
    "#                 schema.append(bigquery.SchemaField(key, \"JSON\"))\n",
    "#             elif isinstance(val, list):\n",
    "#                 schema.append(bigquery.SchemaField(key, \"STRING\", mode=\"REPEATED\"))\n",
    "#             elif isinstance(val, dict):\n",
    "#                 schema.append(bigquery.SchemaField(key, \"JSON\"))\n",
    "#             elif isinstance(val, str):\n",
    "#                 schema.append(bigquery.SchemaField(key, \"STRING\"))\n",
    "#             else:\n",
    "#                 schema.append(bigquery.SchemaField(key, \"STRING\"))\n",
    "#         return schema\n",
    "\n",
    "#     def export_to_bq(self, stories, dataset_id=\"stories_dataset\", table_id=\"user_stories\"):\n",
    "#         if not stories:\n",
    "#             print(\"No stories to export\")\n",
    "#             return\n",
    "\n",
    "#         client = bigquery.Client(project=self.project_id)\n",
    "#         table_ref = f\"{self.project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "#         schema = self._infer_bq_schema(stories[0])\n",
    "\n",
    "#         # Ensure dataset\n",
    "#         try:\n",
    "#             client.get_dataset(dataset_id)\n",
    "#         except Exception:\n",
    "#             dataset = bigquery.Dataset(f\"{self.project_id}.{dataset_id}\")\n",
    "#             dataset.location = self.location\n",
    "#             client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "#         # Ensure table\n",
    "#         try:\n",
    "#             client.get_table(table_ref)\n",
    "#         except Exception:\n",
    "#             table = bigquery.Table(table_ref, schema=schema)\n",
    "#             client.create_table(table)\n",
    "\n",
    "#         # Stringify dict fields (BQ JSON is fine, but this is robust)\n",
    "#         rows = []\n",
    "#         for story in stories:\n",
    "#             row = story.copy()\n",
    "#             for k, v in row.items():\n",
    "#                 if isinstance(v, dict):\n",
    "#                     row[k] = json.dumps(v, ensure_ascii=False)\n",
    "#             rows.append(row)\n",
    "\n",
    "#         print(\"⬆️ Exporting to BigQuery...\")\n",
    "#         errors = client.insert_rows_json(table_ref, rows)\n",
    "#         if errors:\n",
    "#             print(f\"❌ Errors inserting rows: {errors}\")\n",
    "#         else:\n",
    "#             print(f\"✅ Inserted {len(rows)} stories into {table_ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a5d896-8ccd-4c49-9b65-93c8f0aff6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Parsing document...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Normalizing PDF pages & building RAG index...\n",
      "🧩 Segmenting requirements...\n",
      "📌 Found requirements: 170\n",
      "⚡ TEST mode active → using only 10 requirements\n",
      "🧠 Generating stories (LLM)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896719b5b89748fbba4cdfbc8742269b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Requirement chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6e8732f9c54d0ebf0eaf9062c6b118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Generated stories (pre-alignment, pre-dedupe): 10\n",
      "🔎 Checking alignment with citations...\n",
      "✅ Aligned: 2 | 🚩 Needs review: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e92f7a6c274318b744d8e181948d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Duplicate check:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e883f0d3b44272b70c0ba9f250f969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clustering dupes:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final stories after dedupe: 10\n",
      "✅ Extracted user stories saved to generated_user_stories.json\n",
      "✅ Saved requirements.json and stories.json\n"
     ]
    }
   ],
   "source": [
    "LLM_MODEL = \"gemini-2.0-flash\"  # e.g., \"gemini-1.5-pro\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "import asyncio\n",
    "import docx\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "from typing import List, Iterable, Dict, Any, Optional\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from requirement_builder import HealthcareStoryExtractor\n",
    "from google.cloud import bigquery\n",
    "from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
    "# Auto-detect project from your auth context\n",
    "bq_client = bigquery.Client()\n",
    "PROJECT_ID = bq_client.project\n",
    "TEST = True   # set this to False for full run\n",
    "\n",
    "# Configs (override via env)\n",
    "FILE_PATH = os.environ.get(\"INPUT_FILE\", \"data/srs.pdf\")\n",
    "OUTPUT_JSON = os.environ.get(\"OUTPUT_JSON\", \"generated_user_stories.json\")\n",
    "DEDUPE = os.environ.get(\"DEDUPE\", \"true\").lower() in {\"1\", \"true\", \"yes\"}\n",
    "DUP_THRESHOLD = float(os.environ.get(\"DUP_THRESHOLD\", \"0.99\"))\n",
    "EXPORT = os.environ.get(\"EXPORT_TO_BQ\", \"false\").lower() in {\"1\", \"true\", \"yes\"}\n",
    "BATCH_LLM_SIZE = int(os.environ.get(\"BATCH_LLM_SIZE\", \"20\"))\n",
    "LLM_INNER_BATCH = int(os.environ.get(\"LLM_INNER_BATCH\", \"5\"))\n",
    "\n",
    "extractor = HealthcareStoryExtractor(project_id=PROJECT_ID)\n",
    "\n",
    "# Run extraction\n",
    "stories = await  extractor.extract_from_file(\n",
    "        FILE_PATH,\n",
    "        dedupe=DEDUPE,\n",
    "        dup_threshold=DUP_THRESHOLD,\n",
    "        batch_llm_size=BATCH_LLM_SIZE,\n",
    "        llm_inner_batch=LLM_INNER_BATCH,\n",
    "    TEST=TEST\n",
    "    \n",
    "    )\n",
    "# Save test cases to CSV\n",
    "# export_testcases_csv(stories, extractor._last_requirements, out_csv=\"testcases.csv\")\n",
    "\n",
    "\n",
    "# Save to JSON\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stories, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✅ Extracted user stories saved to {OUTPUT_JSON}\")\n",
    "\n",
    "import json\n",
    "\n",
    "# Save segmented requirements\n",
    "with open(\"requirements.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extractor._last_requirements, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save generated user stories\n",
    "with open(\"stories.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stories, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Saved requirements.json and stories.json\")\n",
    "\n",
    "# Optional: export to BigQuery\n",
    "# if EXPORT:\n",
    "#     extractor.export_to_bq(stories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dc1868a-6185-4ba9-8f48-bd17d02002a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform langchain-google-vertexai PyPDF2 python-docx pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22579604-3019-4851-ba1a-bd3307afaa6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5182130-0a37-48a1-8b09-8196ca148f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================== Layer-3: Test Generation ==========================\n",
    "# Generates:\n",
    "#   - Gherkin .feature files (1 Scenario per AC) with tags: @priority_*, @req_*, @HIPAA...\n",
    "#   - Step-definition stubs (pytest-bdd or behave) with E2E placeholders\n",
    "#   - RTM coverage CSV: requirement_id ↔ story_id ↔ scenario_id\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "_SAFE = re.compile(r\"[^A-Za-z0-9._-]+\")\n",
    "\n",
    "def _safe_name(s: str, default: str = \"item\") -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return default\n",
    "    s = re.sub(_SAFE, \"_\", s)\n",
    "    return s[:80] or default  # keep filenames short-ish\n",
    "\n",
    "def _ensure_dir(path: str | Path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _detect_compliance_tags(story: Dict[str, Any]) -> list[str]:\n",
    "    \"\"\"Return compliance tags like HIPAA, FDA21CFR11 if mentioned in non_functional or citations.\"\"\"\n",
    "    tags = set()\n",
    "    text_blobs = []\n",
    "    for nfr in story.get(\"non_functional\") or []:\n",
    "        text_blobs.append(nfr or \"\")\n",
    "    for c in story.get(\"citations\") or []:\n",
    "        text_blobs.append((c or {}).get(\"snippet\", \"\") or \"\")\n",
    "    blob = \" \".join(text_blobs).lower()\n",
    "    if \"hipaa\" in blob:\n",
    "        tags.add(\"HIPAA\")\n",
    "    if \"21 cfr part 11\" in blob or \"fda 21 cfr part 11\" in blob or \"21cfr part 11\" in blob:\n",
    "        tags.add(\"FDA21CFR11\")\n",
    "    if \"iso 13485\" in blob:\n",
    "        tags.add(\"ISO13485\")\n",
    "    if \"iec 62304\" in blob:\n",
    "        tags.add(\"IEC62304\")\n",
    "    if \"iso 27001\" in blob:\n",
    "        tags.add(\"ISO27001\")\n",
    "    return sorted(tags)\n",
    "\n",
    "def _story_scenarios(story: Dict[str, Any]) -> List[tuple[str, Dict[str, str]]]:\n",
    "    \"\"\"Return [(scenario_id, ac_dict), ...] where ac_dict has given/when/then.\"\"\"\n",
    "    out: List[tuple[str, Dict[str, str]]] = []\n",
    "    acs = story.get(\"acceptance_criteria\") or []\n",
    "    for idx, ac in enumerate(acs, 1):\n",
    "        sid = f\"{story.get('story_id','S')}_AC{idx}\"\n",
    "        out.append((sid, ac))\n",
    "    return out\n",
    "\n",
    "def _extract_requirements(story: Dict[str, Any]) -> List[str]:\n",
    "    r = story.get(\"source_requirement_ids\") or []\n",
    "    return [re.sub(_SAFE, \"_\", str(x))[:80] for x in r if x]\n",
    "\n",
    "def _priority_tag(priority: str | None) -> str:\n",
    "    p = (priority or \"\").strip() or \"Unspecified\"\n",
    "    p = re.sub(r\"\\s+\", \"\", p)\n",
    "    return f\"priority_{p}\"\n",
    "\n",
    "def export_gherkin_features(\n",
    "    stories: List[Dict[str, Any]],\n",
    "    out_dir: str = \"features\",\n",
    "    feature_per_epic: bool = True,\n",
    ") -> List[Path]:\n",
    "    \"\"\"\n",
    "    Write .feature files from stories.\n",
    "    - Group by 'epic' (default) OR one file per story if feature_per_epic=False\n",
    "    - One Scenario per AC\n",
    "    - Tags: @priority_*, @req_REQ-123, @HIPAA, @FDA21CFR11, ...\n",
    "    \"\"\"\n",
    "    _ensure_dir(out_dir)\n",
    "\n",
    "    groups: dict[str, list[Dict[str, Any]]] = defaultdict(list)\n",
    "    if feature_per_epic:\n",
    "        for s in stories:\n",
    "            key = s.get(\"epic\") or \"General\"\n",
    "            groups[key].append(s)\n",
    "    else:\n",
    "        for s in stories:\n",
    "            key = s.get(\"story_id\") or \"Story\"\n",
    "            groups[key].append(s)\n",
    "\n",
    "    files: List[Path] = []\n",
    "    for group_key, group_stories in groups.items():\n",
    "        fname = _safe_name(group_key or \"feature\")\n",
    "        path = Path(out_dir) / f\"{fname}.feature\"\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Feature: {group_key or 'User Stories'}\\n\\n\")\n",
    "            for s in group_stories:\n",
    "                # Comment with the story text (nice for humans)\n",
    "                f.write(f\"  # {s.get('user_story','')}\\n\")\n",
    "\n",
    "                # Build base tags\n",
    "                tags = [f\"@{_priority_tag(s.get('priority'))}\"]\n",
    "                # Requirement tags\n",
    "                for rid in _extract_requirements(s):\n",
    "                    tags.append(f\"@req_{rid}\")\n",
    "                # Compliance tags\n",
    "                for t in _detect_compliance_tags(s):\n",
    "                    tags.append(f\"@{t}\")\n",
    "\n",
    "                # Emit Scenarios (one per AC)\n",
    "                for scenario_id, ac in _story_scenarios(s):\n",
    "                    f.write(\"  \" + \" \".join(tags) + \"\\n\")\n",
    "                    scen_title = _safe_name(scenario_id, \"Scenario\")\n",
    "                    f.write(f\"  Scenario: {scen_title}\\n\")\n",
    "                    f.write(f\"    Given {ac.get('given','<given TBD>')}\\n\")\n",
    "                    f.write(f\"    When {ac.get('when','<when TBD>')}\\n\")\n",
    "                    f.write(f\"    Then {ac.get('then','<then TBD>')}\\n\\n\")\n",
    "\n",
    "        files.append(path)\n",
    "\n",
    "    print(f\"🧪 Wrote {len(files)} Gherkin feature file(s) to {out_dir}\")\n",
    "    return files\n",
    "\n",
    "# -------- Step Stubs (pytest-bdd or behave) --------\n",
    "\n",
    "_PYTEST_BDD_TEMPLATE = \"\"\"# Auto-generated pytest-bdd step definitions.\n",
    "# Run: pytest -k feature\n",
    "import pytest\n",
    "from pytest_bdd import given, when, then, scenarios\n",
    "\n",
    "# Link feature(s)\n",
    "scenarios(\"{feature_glob}\")\n",
    "\n",
    "# Example shared test data (E2E placeholders)\n",
    "TEST_CONTEXT = {{\n",
    "    \"patient_id\": \"PAT-001\",\n",
    "    \"session_id\": \"SES-001\",\n",
    "    \"clinician_id\": \"DOC-123\",\n",
    "}}\n",
    "\n",
    "@given(\"{given_text}\")\n",
    "def step_given():\n",
    "    # TODO: implement setup for: {given_text}\n",
    "    # e.g., create patient in DB using TEST_CONTEXT[\"patient_id\"]\n",
    "    pass\n",
    "\n",
    "@when(\"{when_text}\")\n",
    "def step_when():\n",
    "    # TODO: implement action for: {when_text}\n",
    "    # e.g., call API to sign in/out, upload document, etc.\n",
    "    pass\n",
    "\n",
    "@then(\"{then_text}\")\n",
    "def step_then():\n",
    "    # TODO: implement assertion for: {then_text}\n",
    "    # e.g., assert response.status_code == 200 or record exists in DB\n",
    "    pass\n",
    "\"\"\"\n",
    "\n",
    "_BEHAVE_TEMPLATE = \"\"\"# Auto-generated behave step definitions.\n",
    "# Run: behave -i {feature_glob}\n",
    "from behave import given, when, then\n",
    "\n",
    "# Example shared test data (E2E placeholders)\n",
    "TEST_CONTEXT = {{\n",
    "    \"patient_id\": \"PAT-001\",\n",
    "    \"session_id\": \"SES-001\",\n",
    "    \"clinician_id\": \"DOC-123\",\n",
    "}}\n",
    "\n",
    "@given('{given_text}')\n",
    "def step_impl_given(context):\n",
    "    # TODO: implement setup for: {given_text}\n",
    "    pass\n",
    "\n",
    "@when('{when_text}')\n",
    "def step_impl_when(context):\n",
    "    # TODO: implement action for: {when_text}\n",
    "    pass\n",
    "\n",
    "@then('{then_text}')\n",
    "def step_impl_then(context):\n",
    "    # TODO: implement assertion for: {then_text}\n",
    "    pass\n",
    "\"\"\"\n",
    "\n",
    "def export_step_stubs(\n",
    "    stories: List[Dict[str, Any]],\n",
    "    out_dir: str = \"steps\",\n",
    "    framework: str = \"pytest-bdd\",  # or \"behave\"\n",
    "    feature_glob: str = \"features/*.feature\",\n",
    ") -> Path:\n",
    "    \"\"\"Produce a single step file with example stubs and E2E placeholders.\"\"\"\n",
    "    _ensure_dir(out_dir)\n",
    "\n",
    "    # Pick first non-empty AC as examples\n",
    "    given_text = when_text = then_text = \"TBD\"\n",
    "    for s in stories:\n",
    "        for _, ac in _story_scenarios(s):\n",
    "            given_text = ac.get(\"given\") or \"a precondition\"\n",
    "            when_text = ac.get(\"when\") or \"an action occurs\"\n",
    "            then_text = ac.get(\"then\") or \"an expected result\"\n",
    "            break\n",
    "        if given_text != \"TBD\":\n",
    "            break\n",
    "\n",
    "    if framework.lower() == \"behave\":\n",
    "        body = _BEHAVE_TEMPLATE.format(\n",
    "            feature_glob=feature_glob,\n",
    "            given_text=given_text.replace('\"', '\\\\\"'),\n",
    "            when_text=when_text.replace('\"', '\\\\\"'),\n",
    "            then_text=then_text.replace('\"', '\\\\\"'),\n",
    "        )\n",
    "        path = Path(out_dir) / \"steps_behave.py\"\n",
    "    else:\n",
    "        body = _PYTEST_BDD_TEMPLATE.format(\n",
    "            feature_glob=feature_glob,\n",
    "            given_text=given_text,\n",
    "            when_text=when_text,\n",
    "            then_text=then_text,\n",
    "        )\n",
    "        path = Path(out_dir) / \"test_steps_bdd.py\"\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(body)\n",
    "\n",
    "    print(f\"🧩 Wrote step stubs for {framework} to {path}\")\n",
    "    return path\n",
    "\n",
    "# -------- Coverage Matrix (RTM) --------\n",
    "\n",
    "def _build_rtm_rows(stories: List[Dict[str, Any]]) -> List[List[str]]:\n",
    "    \"\"\"Rows: requirement_id, story_id, epic, priority, scenario_id, tags, pages\"\"\"\n",
    "    rows: List[List[str]] = []\n",
    "    for s in stories:\n",
    "        pages = sorted({c.get(\"page\") for c in (s.get(\"citations\") or []) if isinstance(c.get(\"page\"), int)})\n",
    "        tags = [f\"@{_priority_tag(s.get('priority'))}\"] + [f\"@req_{rid}\" for rid in _extract_requirements(s)]\n",
    "        tags += [f\"@{t}\" for t in _detect_compliance_tags(s)]\n",
    "\n",
    "        scenarios = _story_scenarios(s)\n",
    "        if scenarios:\n",
    "            for scen_id, _ in scenarios:\n",
    "                rows.append([\n",
    "                    \";\".join(s.get(\"source_requirement_ids\") or [\"-\"]),\n",
    "                    s.get(\"story_id\", \"\"),\n",
    "                    s.get(\"epic\", \"\"),\n",
    "                    s.get(\"priority\", \"\"),\n",
    "                    scen_id,\n",
    "                    \" \".join(tags),\n",
    "                    \";\".join(map(str, pages)) if pages else \"\",\n",
    "                ])\n",
    "        else:\n",
    "            rows.append([\n",
    "                \";\".join(s.get(\"source_requirement_ids\") or [\"-\"]),\n",
    "                s.get(\"story_id\", \"\"),\n",
    "                s.get(\"epic\", \"\"),\n",
    "                s.get(\"priority\", \"\"),\n",
    "                \"\",  # no scenario\n",
    "                \" \".join(tags),\n",
    "                \";\".join(map(str, pages)) if pages else \"\",\n",
    "            ])\n",
    "    return rows\n",
    "\n",
    "def export_traceability_csv(stories: List[Dict[str, Any]], path: str = \"traceability.csv\") -> Path:\n",
    "    rows = _build_rtm_rows(stories)\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"requirement_id\", \"story_id\", \"epic\", \"priority\", \"scenario_id\", \"tags\", \"pages\"])\n",
    "        w.writerows(rows)\n",
    "    print(f\"📊 Wrote RTM to {path} ({len(rows)} rows)\")\n",
    "    return Path(path)\n",
    "\n",
    "def flag_requirements_with_no_scenarios(stories: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"Return list of requirement IDs that do not map to any Scenario.\"\"\"\n",
    "    req_to_scen: dict[str, int] = defaultdict(int)\n",
    "    for s in stories:\n",
    "        rids = s.get(\"source_requirement_ids\") or [\"-\"]\n",
    "        scen_count = len(_story_scenarios(s))\n",
    "        for rid in rids:\n",
    "            req_to_scen[rid] += scen_count\n",
    "    return sorted([rid for rid, n in req_to_scen.items() if n == 0])\n",
    "\n",
    "def generate_tests_from_stories(\n",
    "    stories: List[Dict[str, Any]],\n",
    "    feature_dir: str = \"features\",\n",
    "    steps_dir: str = \"steps\",\n",
    "    framework: str = \"pytest-bdd\",  # or \"behave\"\n",
    "    feature_per_epic: bool = True,\n",
    "    traceability_csv: str = \"traceability.csv\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"One-call orchestrator for Layer-3 outputs.\"\"\"\n",
    "    feature_files = export_gherkin_features(stories, out_dir=feature_dir, feature_per_epic=feature_per_epic)\n",
    "    step_file = export_step_stubs(stories, out_dir=steps_dir, framework=framework, feature_glob=f\"{feature_dir}/*.feature\")\n",
    "    rtm_file = export_traceability_csv(stories, path=traceability_csv)\n",
    "    gaps = flag_requirements_with_no_scenarios(stories)\n",
    "\n",
    "    if gaps:\n",
    "        print(f\"⚠️ Requirements with 0 scenarios: {gaps}\")\n",
    "    else:\n",
    "        print(\"✅ All requirements have at least one scenario.\")\n",
    "\n",
    "    return {\n",
    "        \"feature_files\": [str(p) for p in feature_files],\n",
    "        \"step_file\": str(step_file),\n",
    "        \"rtm_csv\": str(rtm_file),\n",
    "        \"gaps\": gaps,\n",
    "    }\n",
    "# ======================== End Layer-3: Test Generation ========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22bc7a81-af32-4c21-a542-95a45b0fe538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================== Layer-3: CSV Exporters (Jira & ADO) ==========================\n",
    "# Creates CSVs ready to import into Jira test plugins (Xray/Zephyr) and ADO Test Plans.\n",
    "\n",
    "def _flatten_scenarios(stories: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Yields rows: {\n",
    "      requirement_id, story_id, epic, priority, scenario_id,\n",
    "      given, when, then, tags (space-delimited), pages (semicolon-delimited)\n",
    "    }\n",
    "    Each AC (Given/When/Then) becomes ONE scenario row.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for s in stories:\n",
    "        rids = s.get(\"source_requirement_ids\") or [\"-\"]\n",
    "        pages = sorted({c.get(\"page\") for c in (s.get(\"citations\") or []) if isinstance(c.get(\"page\"), int)})\n",
    "        tags = [f\"@{_priority_tag(s.get('priority'))}\"] + [f\"@req_{rid}\" for rid in _extract_requirements(s)]\n",
    "        tags += [f\"@{t}\" for t in _detect_compliance_tags(s)]\n",
    "\n",
    "        scenarios = _story_scenarios(s)\n",
    "        if not scenarios:\n",
    "            # still emit a row with empty scenario so importers can see the gap\n",
    "            rows.append({\n",
    "                \"requirement_id\": \";\".join(rids),\n",
    "                \"story_id\": s.get(\"story_id\", \"\"),\n",
    "                \"epic\": s.get(\"epic\", \"\"),\n",
    "                \"priority\": s.get(\"priority\", \"\"),\n",
    "                \"scenario_id\": \"\",\n",
    "                \"given\": \"\",\n",
    "                \"when\": \"\",\n",
    "                \"then\": \"\",\n",
    "                \"tags\": \" \".join(tags),\n",
    "                \"pages\": \";\".join(map(str, pages)) if pages else \"\",\n",
    "                \"user_story\": s.get(\"user_story\", \"\"),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for scen_id, ac in scenarios:\n",
    "            rows.append({\n",
    "                \"requirement_id\": \";\".join(rids),\n",
    "                \"story_id\": s.get(\"story_id\", \"\"),\n",
    "                \"epic\": s.get(\"epic\", \"\"),\n",
    "                \"priority\": s.get(\"priority\", \"\"),\n",
    "                \"scenario_id\": scen_id,\n",
    "                \"given\": (ac or {}).get(\"given\", \"\"),\n",
    "                \"when\": (ac or {}).get(\"when\", \"\"),\n",
    "                \"then\": (ac or {}).get(\"then\", \"\"),\n",
    "                \"tags\": \" \".join(tags),\n",
    "                \"pages\": \";\".join(map(str, pages)) if pages else \"\",\n",
    "                \"user_story\": s.get(\"user_story\", \"\"),\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "\n",
    "def export_to_jira_csv(\n",
    "    stories: List[Dict[str, Any]],\n",
    "    path: str = \"jira_testcases.csv\",\n",
    "    project_key: str | None = None,\n",
    "    default_labels: list[str] | None = None,\n",
    "    test_type: str = \"Manual\",     # Xray/Zephyr friendly\n",
    "):\n",
    "    \"\"\"\n",
    "    Produce a CSV that works well with Jira test plugins (Xray/Zephyr/TM4J) via CSV import.\n",
    "    One row per Scenario (AC). Map fields during import:\n",
    "      - Issue Type        -> \"Test\"\n",
    "      - Project Key       -> your project (if not in file, select in UI)\n",
    "      - Summary           -> Test Case title\n",
    "      - Priority          -> Priority\n",
    "      - Labels            -> space/comma separated\n",
    "      - Requirement Keys  -> requirement_id (map to your \"Requirement Link\" field)\n",
    "      - Test Type         -> Manual\n",
    "      - Step Action       -> Given + When\n",
    "      - Step Result       -> Then\n",
    "      - Description       -> (optional) the original user story\n",
    "\n",
    "    NOTE: Different Jira plugins have slightly different CSV headers. These columns are broadly compatible.\n",
    "    You can re-map columns during the import wizard.\n",
    "    \"\"\"\n",
    "    default_labels = default_labels or [\"auto-generated\", \"vertex-ai\", \"traceable\"]\n",
    "    rows = _flatten_scenarios(stories)\n",
    "\n",
    "    headers = [\n",
    "        \"Issue Type\", \"Project Key\", \"Summary\", \"Priority\", \"Labels\",\n",
    "        \"Requirement Keys\", \"Test Type\", \"Step Action\", \"Step Data\", \"Step Result\",\n",
    "        \"Description\", \"Tags\", \"Pages\", \"Story Id\", \"Epic\"\n",
    "    ]\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            summary = r[\"user_story\"][:255] if r[\"user_story\"] else (r[\"scenario_id\"] or \"Generated Test\")\n",
    "            step_action = \" | \".join([x for x in [r[\"given\"], r[\"when\"]] if x])\n",
    "            step_result = r[\"then\"]\n",
    "            labels_str = \",\".join(default_labels)\n",
    "            req_keys = r[\"requirement_id\"]  # If your requirements already exist in Jira, use their keys here\n",
    "\n",
    "            w.writerow([\n",
    "                \"Test\",\n",
    "                project_key or \"\",      # leave blank to pick in UI\n",
    "                summary,\n",
    "                r[\"priority\"] or \"\",\n",
    "                labels_str,\n",
    "                req_keys,\n",
    "                test_type,\n",
    "                step_action,\n",
    "                \"\",                     # Step Data (optional)\n",
    "                step_result,\n",
    "                r[\"user_story\"],\n",
    "                r[\"tags\"],\n",
    "                r[\"pages\"],\n",
    "                r[\"story_id\"],\n",
    "                r[\"epic\"],\n",
    "            ])\n",
    "\n",
    "    print(f\"🗂️  Wrote Jira-friendly CSV to {path}\")\n",
    "    return Path(path)\n",
    "\n",
    "\n",
    "def export_to_ado_csv(\n",
    "    stories: List[Dict[str, Any]],\n",
    "    path: str = \"ado_testcases.csv\",\n",
    "    area_path: str | None = None,\n",
    "    iteration_path: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Produce an Azure DevOps Test Plans friendly CSV.\n",
    "    Recommended field mapping in ADO import:\n",
    "      - \"Test Case Title\"    -> Title of the test\n",
    "      - \"Step Action\"        -> Given + When\n",
    "      - \"Step Expected\"      -> Then\n",
    "      - \"Requirement ID\"     -> requirement_id\n",
    "      - \"Priority\"           -> Priority\n",
    "      - \"Tags\"               -> Tags\n",
    "      - \"Area Path\"          -> (optional) area_path (string)\n",
    "      - \"Iteration Path\"     -> (optional) iteration_path (string)\n",
    "\n",
    "    One row per Scenario (AC).\n",
    "    \"\"\"\n",
    "    rows = _flatten_scenarios(stories)\n",
    "\n",
    "    headers = [\n",
    "        \"Test Case Title\", \"Step Action\", \"Step Expected\",\n",
    "        \"Requirement ID\", \"Priority\", \"Tags\", \"Pages\",\n",
    "        \"Story Id\", \"Epic\", \"Area Path\", \"Iteration Path\"\n",
    "    ]\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(headers)\n",
    "        for r in rows:\n",
    "            title = r[\"user_story\"][:255] if r[\"user_story\"] else (r[\"scenario_id\"] or \"Generated Test\")\n",
    "            step_action = \" | \".join([x for x in [r[\"given\"], r[\"when\"]] if x])\n",
    "            step_expected = r[\"then\"]\n",
    "\n",
    "            w.writerow([\n",
    "                title,\n",
    "                step_action,\n",
    "                step_expected,\n",
    "                r[\"requirement_id\"],\n",
    "                r[\"priority\"] or \"\",\n",
    "                r[\"tags\"],\n",
    "                r[\"pages\"],\n",
    "                r[\"story_id\"],\n",
    "                r[\"epic\"],\n",
    "                area_path or \"\",\n",
    "                iteration_path or \"\",\n",
    "            ])\n",
    "\n",
    "    print(f\"🗂️  Wrote ADO-friendly CSV to {path}\")\n",
    "    return Path(path)\n",
    "# ====================== End Layer-3: CSV Exporters (Jira & ADO) ======================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00564258-6777-4604-b13b-f4b73d19c638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testcase_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtestcase_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestCaseGenerator\n\u001b[0;32m----> 3\u001b[0m tcgen \u001b[38;5;241m=\u001b[39m \u001b[43mtestcase_generator\u001b[49m()\n\u001b[1;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m tcgen\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      5\u001b[0m     stories,\n\u001b[1;32m      6\u001b[0m     feature_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     traceability_csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceability.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testcase_generator' is not defined"
     ]
    }
   ],
   "source": [
    "from testcase_generator import TestCaseGenerator\n",
    "\n",
    "tcgen = testcase_generator()\n",
    "result = tcgen.generate(\n",
    "    stories,\n",
    "    feature_dir=\"features\",\n",
    "    steps_dir=\"steps\",\n",
    "    framework=\"pytest-bdd\",     # or \"behave\"\n",
    "    feature_per_epic=True,\n",
    "    traceability_csv=\"traceability.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79783e62-3230-457e-8a38-1e37bd782103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583b901-0243-410c-95fb-bf4540302d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d6972a1-965f-4b9f-b5d6-57abedea82de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Wrote 6 Gherkin feature file(s) to features\n",
      "🧩 Wrote step stubs for pytest-bdd to steps/test_steps_bdd.py\n",
      "📊 Wrote RTM to traceability.csv (187 rows)\n",
      "✅ All requirements have at least one scenario.\n",
      "🗂️  Wrote Jira-friendly CSV to jira_testcases.csv\n",
      "🗂️  Wrote ADO-friendly CSV to ado_testcases.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('ado_testcases.csv')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After extraction + (optional) dedupe:\n",
    "# stories = asyncio.run(extractor.extract_from_file(FILE_PATH, ...))\n",
    "\n",
    "# 1) Generate features/steps/RTM as before\n",
    "result = generate_tests_from_stories(\n",
    "    stories,\n",
    "    feature_dir=\"features\",\n",
    "    steps_dir=\"steps\",\n",
    "    framework=\"pytest-bdd\",     # or \"behave\"\n",
    "    feature_per_epic=True,\n",
    "    traceability_csv=\"traceability.csv\",\n",
    ")\n",
    "\n",
    "# 2) Create CSVs for Jira and ADO imports\n",
    "export_to_jira_csv(\n",
    "    stories,\n",
    "    path=\"jira_testcases.csv\",\n",
    "    project_key=\"\",                 # put your Jira project key or leave blank and select in UI\n",
    "    default_labels=[\"auto-generated\",\"vertex-ai\",\"traceable\"],\n",
    "    test_type=\"Manual\",\n",
    ")\n",
    "\n",
    "export_to_ado_csv(\n",
    "    stories,\n",
    "    path=\"ado_testcases.csv\",\n",
    "    area_path=\"Healthcare\\\\DayHealth\",   # optional\n",
    "    iteration_path=\"Release 1\",          # optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47d48294-d30b-43d6-92b6-6bb27630e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Coverage matrix generated: coverage_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Coverage Matrix Generator (Enhanced)\n",
    "------------------------------------\n",
    "Merges Requirements ↔ Stories ↔ Test Cases into one CSV/Excel\n",
    "with coverage flags and citations for traceability.\n",
    "\n",
    "Inputs:\n",
    "  - requirements.json\n",
    "  - stories.json\n",
    "  - testcases.csv\n",
    "\n",
    "Output:\n",
    "  - coverage_matrix.csv\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------ Load Inputs ------------------\n",
    "reqs = json.load(open(\"requirements.json\", \"r\", encoding=\"utf-8\"))\n",
    "stories = json.load(open(\"stories.json\", \"r\", encoding=\"utf-8\"))\n",
    "testcases = pd.read_csv(\"testcases.csv\")\n",
    "\n",
    "# Normalize requirements\n",
    "df_reqs = pd.DataFrame(reqs)\n",
    "if \"epic\" not in df_reqs.columns:\n",
    "    df_reqs[\"epic\"] = \"General\"\n",
    "\n",
    "df_reqs = df_reqs.rename(columns={\n",
    "    \"req_id\": \"Requirement ID\",\n",
    "    \"text\": \"Requirement Text\",\n",
    "    \"epic\": \"Epic (Req)\"\n",
    "})\n",
    "\n",
    "# Normalize stories\n",
    "df_stories = pd.DataFrame(stories)\n",
    "df_stories[\"Story Id\"] = df_stories[\"story_id\"]  # for join with testcases\n",
    "\n",
    "# Ensure source req IDs exist\n",
    "df_stories[\"source_requirement_ids\"] = df_stories[\"source_requirement_ids\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "# Flatten citations (page numbers + snippet preview)\n",
    "def format_citations(cites):\n",
    "    if not cites:\n",
    "        return \"\"\n",
    "    return \"; \".join([f\"p{c.get('page')}:{c.get('snippet','')[:80]}\" for c in cites])\n",
    "\n",
    "df_stories[\"Citations\"] = df_stories[\"citations\"].apply(format_citations)\n",
    "\n",
    "# Explode stories by requirement ID\n",
    "rows = []\n",
    "for _, s in df_stories.iterrows():\n",
    "    for rid in s[\"source_requirement_ids\"]:\n",
    "        rows.append({\n",
    "            \"Requirement ID\": rid,\n",
    "            \"Story Id\": s[\"story_id\"],\n",
    "            \"User Story\": s[\"user_story\"],\n",
    "            \"Epic (Story)\": s.get(\"epic\", \"\"),\n",
    "            \"Alignment Score\": s.get(\"alignment_score\", 0.0),\n",
    "            \"Needs Review\": s.get(\"needs_review\", False),\n",
    "            \"Citations\": s.get(\"Citations\", \"\")\n",
    "        })\n",
    "df_map = pd.DataFrame(rows)\n",
    "\n",
    "# Count test cases per story\n",
    "tc_counts = testcases.groupby(\"Story Id\").size().reset_index(name=\"Test Case Count\")\n",
    "\n",
    "# Merge requirements ↔ stories ↔ testcases\n",
    "matrix = (\n",
    "    df_reqs\n",
    "    .merge(df_map, on=\"Requirement ID\", how=\"left\")\n",
    "    .merge(tc_counts, on=\"Story Id\", how=\"left\")\n",
    "    .fillna({\"Test Case Count\": 0})\n",
    ")\n",
    "\n",
    "# Coverage classification\n",
    "def classify(row):\n",
    "    has_story = pd.notna(row[\"Story Id\"])\n",
    "    has_test = row[\"Test Case Count\"] > 0\n",
    "\n",
    "    if has_story and has_test:\n",
    "        return \"✅ Covered\"\n",
    "    elif has_story and not has_test:\n",
    "        return \"⚠️ No tests\"\n",
    "    elif not has_story:\n",
    "        # if requirement text exists but no story mapped\n",
    "        return \"⚠️ No stories\"\n",
    "    else:\n",
    "        return \"❌ Missing everything\"\n",
    "\n",
    "matrix[\"Coverage Status\"] = matrix.apply(classify, axis=1)\n",
    "\n",
    "# Save outputs\n",
    "matrix.to_csv(\"coverage_matrix.csv\", index=False, encoding=\"utf-8\")\n",
    "# matrix.to_excel(\"coverage_matrix.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ Coverage matrix generated: coverage_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcdfe101-dbbe-4249-9218-2452dd08520a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Requirement-level coverage: coverage_matrix.csv\n",
      "✅ Epic-level rollup: epic_coverage.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Coverage Matrix + Epic Rollup\n",
    "-----------------------------\n",
    "Generates requirement-level coverage matrix and\n",
    "aggregated coverage metrics per Epic.\n",
    "\n",
    "Inputs:\n",
    "  - requirements.json\n",
    "  - stories.json\n",
    "  - testcases.csv\n",
    "\n",
    "Outputs:\n",
    "  - coverage_matrix.csv\n",
    "  - epic_coverage.csv\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------ Load Inputs ------------------\n",
    "reqs = json.load(open(\"requirements.json\", \"r\", encoding=\"utf-8\"))\n",
    "stories = json.load(open(\"stories.json\", \"r\", encoding=\"utf-8\"))\n",
    "testcases = pd.read_csv(\"testcases.csv\")\n",
    "\n",
    "# ------------------ Normalize Requirements ------------------\n",
    "df_reqs = pd.DataFrame(reqs)\n",
    "if \"epic\" not in df_reqs.columns:\n",
    "    df_reqs[\"epic\"] = \"General\"\n",
    "\n",
    "df_reqs = df_reqs.rename(columns={\n",
    "    \"req_id\": \"Requirement ID\",\n",
    "    \"text\": \"Requirement Text\",\n",
    "    \"epic\": \"Epic (Req)\"\n",
    "})\n",
    "\n",
    "# ------------------ Normalize Stories ------------------\n",
    "df_stories = pd.DataFrame(stories)\n",
    "df_stories[\"Story Id\"] = df_stories[\"story_id\"]\n",
    "\n",
    "df_stories[\"source_requirement_ids\"] = df_stories[\"source_requirement_ids\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "def format_citations(cites):\n",
    "    if not cites:\n",
    "        return \"\"\n",
    "    return \"; \".join([f\"p{c.get('page')}:{c.get('snippet','')[:80]}\" for c in cites])\n",
    "\n",
    "df_stories[\"Citations\"] = df_stories[\"citations\"].apply(format_citations)\n",
    "\n",
    "# Explode stories by requirement ID\n",
    "rows = []\n",
    "for _, s in df_stories.iterrows():\n",
    "    for rid in s[\"source_requirement_ids\"]:\n",
    "        rows.append({\n",
    "            \"Requirement ID\": rid,\n",
    "            \"Story Id\": s[\"story_id\"],\n",
    "            \"User Story\": s[\"user_story\"],\n",
    "            \"Epic (Story)\": s.get(\"epic\", \"\"),\n",
    "            \"Alignment Score\": s.get(\"alignment_score\", 0.0),\n",
    "            \"Needs Review\": s.get(\"needs_review\", False),\n",
    "            \"Citations\": s.get(\"Citations\", \"\")\n",
    "        })\n",
    "df_map = pd.DataFrame(rows)\n",
    "\n",
    "# ------------------ Test Case Mapping ------------------\n",
    "tc_counts = testcases.groupby(\"Story Id\").size().reset_index(name=\"Test Case Count\")\n",
    "\n",
    "# Merge all\n",
    "matrix = (\n",
    "    df_reqs\n",
    "    .merge(df_map, on=\"Requirement ID\", how=\"left\")\n",
    "    .merge(tc_counts, on=\"Story Id\", how=\"left\")\n",
    "    .fillna({\"Test Case Count\": 0})\n",
    ")\n",
    "\n",
    "# Coverage classification\n",
    "def classify(row):\n",
    "    has_story = pd.notna(row[\"Story Id\"])\n",
    "    has_test = row[\"Test Case Count\"] > 0\n",
    "\n",
    "    if has_story and has_test:\n",
    "        return \"✅ Covered\"\n",
    "    elif has_story and not has_test:\n",
    "        return \"⚠️ No tests\"\n",
    "    elif not has_story:\n",
    "        return \"⚠️ No stories\"\n",
    "    else:\n",
    "        return \"❌ Missing everything\"\n",
    "\n",
    "matrix[\"Coverage Status\"] = matrix.apply(classify, axis=1)\n",
    "\n",
    "# ------------------ Epic Rollup ------------------\n",
    "epic_rollup = (\n",
    "    matrix.groupby(\"Epic (Req)\")\n",
    "    .agg(\n",
    "        total_reqs=(\"Requirement ID\", \"nunique\"),\n",
    "        with_stories=(\"Story Id\", lambda x: x.notna().sum()),\n",
    "        with_tests=(\"Test Case Count\", lambda x: (x > 0).sum())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "epic_rollup[\"% Story Coverage\"] = (epic_rollup[\"with_stories\"] / epic_rollup[\"total_reqs\"] * 100).round(1)\n",
    "epic_rollup[\"% Test Coverage\"] = (epic_rollup[\"with_tests\"] / epic_rollup[\"total_reqs\"] * 100).round(1)\n",
    "\n",
    "# ------------------ Save Outputs ------------------\n",
    "matrix.to_csv(\"coverage_matrix.csv\", index=False, encoding=\"utf-8\")\n",
    "epic_rollup.to_csv(\"epic_coverage.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Requirement-level coverage: coverage_matrix.csv\")\n",
    "print(\"✅ Epic-level rollup: epic_coverage.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81c19a32-e9e3-4d5a-811d-3542d82eed9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Compliance Evidence Report: compliance_evidence.csv and compliance_evidence.xlsx\n",
      "   Stories analyzed: 150 | Stories with missing controls: 150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requirement ID</th>\n",
       "      <th>Story Id</th>\n",
       "      <th>Epic</th>\n",
       "      <th>Priority</th>\n",
       "      <th>User Story</th>\n",
       "      <th>Pages (Citations)</th>\n",
       "      <th>Alignment Score</th>\n",
       "      <th>Needs Review</th>\n",
       "      <th>Matched Clauses</th>\n",
       "      <th>Clause Scores</th>\n",
       "      <th>Expected Controls</th>\n",
       "      <th>Detected Controls</th>\n",
       "      <th>Missing Controls</th>\n",
       "      <th>Evidence (Story + Steps)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>7c4c1495-1f6d-4462-bd63-191fd15ea6ba</td>\n",
       "      <td>Digitize patient records and automate tracking.</td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Nurse, I want to access digitized patient...</td>\n",
       "      <td>4;4;6</td>\n",
       "      <td>0.449</td>\n",
       "      <td>False</td>\n",
       "      <td>FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...</td>\n",
       "      <td>0.63; 0.61; 0.601; 0.552</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a Nurse, I want to access digitized patient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>855160d4-16c6-4f25-a186-5279dcbbcfd3</td>\n",
       "      <td>Replace Trillium's current system with a new s...</td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Doctor, I want to access patient data thr...</td>\n",
       "      <td>5;5;6</td>\n",
       "      <td>0.179</td>\n",
       "      <td>False</td>\n",
       "      <td>FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...</td>\n",
       "      <td>0.626; 0.594; 0.58; 0.56</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a Doctor, I want to access patient data thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.1.1</td>\n",
       "      <td>a99367a2-ad05-4a4b-a4d2-91390abb0115</td>\n",
       "      <td>Implement Database for Trillium Health</td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Doctor, I want to access patient clinical...</td>\n",
       "      <td>6;6</td>\n",
       "      <td>0.622</td>\n",
       "      <td>False</td>\n",
       "      <td>FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...</td>\n",
       "      <td>0.612; 0.58; 0.573; 0.56</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a Doctor, I want to access patient clinical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.1.2</td>\n",
       "      <td>5dd0d188-2605-4668-8305-d3155760a7e6</td>\n",
       "      <td>Clinician Portal Development</td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Doctor, I want to access and view patient...</td>\n",
       "      <td>6;13</td>\n",
       "      <td>0.222</td>\n",
       "      <td>False</td>\n",
       "      <td>ISO 13485 4.2.5; FDA 21 CFR Part 11 11.10(e); ...</td>\n",
       "      <td>0.549; 0.549; 0.542; 0.512</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a Doctor, I want to access and view patient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.1.3</td>\n",
       "      <td>bf688cd2-f118-4e36-9bfd-432eebbbf55d</td>\n",
       "      <td></td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Patient, I want to check in to Day Health...</td>\n",
       "      <td>10;9</td>\n",
       "      <td>0.184</td>\n",
       "      <td>False</td>\n",
       "      <td>FDA 21 CFR Part 11 11.100; FDA 21 CFR Part 11 ...</td>\n",
       "      <td>0.613; 0.608; 0.558; 0.543</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a Patient, I want to check in to Day Health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>5.4.1.1</td>\n",
       "      <td>4246c8f4-e5ad-4a05-a704-fe8c5127152e</td>\n",
       "      <td></td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Clinician, I want to be able to view a pa...</td>\n",
       "      <td>11;6;6;13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "      <td>ISO 13485 4.2.5; ISO 27001 A.9; IEC 62304 5.1;...</td>\n",
       "      <td>0.538; 0.527; 0.521; 0.505</td>\n",
       "      <td>audit_trail; data_integrity; rbac; risk_manage...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; rbac; risk_manage...</td>\n",
       "      <td>As a Clinician, I want to be able to view a pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>5.4.1.2</td>\n",
       "      <td>4b3aa627-8d93-433f-9189-fcd1de80e9c8</td>\n",
       "      <td></td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Clinician, I want the application to comm...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "      <td>ISO 27001 A.10; ISO 27001 A.9; FDA 21 CFR Part...</td>\n",
       "      <td>0.59; 0.561; 0.554; 0.549</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; encr...</td>\n",
       "      <td>encryption</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a Clinician, I want the application to comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>5.4.2</td>\n",
       "      <td>dcca3410-e2cd-48f3-8658-325a412eb531</td>\n",
       "      <td></td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Nurse, I want to update patient vitals, s...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "      <td>FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...</td>\n",
       "      <td>0.633; 0.629; 0.588; 0.541</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a Nurse, I want to update patient vitals, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5.4.3</td>\n",
       "      <td>85a4042f-8972-40b9-8def-ef8d803315ab</td>\n",
       "      <td></td>\n",
       "      <td>Must</td>\n",
       "      <td>As a user, I want to add, modify, or check pat...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "      <td>FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...</td>\n",
       "      <td>0.667; 0.631; 0.628; 0.624</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a user, I want to add, modify, or check pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>4cacda81-286c-46f1-be12-7ff5626221dd</td>\n",
       "      <td></td>\n",
       "      <td>Must</td>\n",
       "      <td>As a Doctor, I want to easily access patient a...</td>\n",
       "      <td>4;16</td>\n",
       "      <td>0.145</td>\n",
       "      <td>True</td>\n",
       "      <td>FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...</td>\n",
       "      <td>0.62; 0.595; 0.592; 0.54</td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td></td>\n",
       "      <td>audit_trail; data_integrity; e_signature; rbac...</td>\n",
       "      <td>As a Doctor, I want to easily access patient a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Requirement ID                              Story Id  \\\n",
       "0              1.1  7c4c1495-1f6d-4462-bd63-191fd15ea6ba   \n",
       "1              1.4  855160d4-16c6-4f25-a186-5279dcbbcfd3   \n",
       "2            2.1.1  a99367a2-ad05-4a4b-a4d2-91390abb0115   \n",
       "3            2.1.2  5dd0d188-2605-4668-8305-d3155760a7e6   \n",
       "4            2.1.3  bf688cd2-f118-4e36-9bfd-432eebbbf55d   \n",
       "..             ...                                   ...   \n",
       "145        5.4.1.1  4246c8f4-e5ad-4a05-a704-fe8c5127152e   \n",
       "146        5.4.1.2  4b3aa627-8d93-433f-9189-fcd1de80e9c8   \n",
       "147          5.4.2  dcca3410-e2cd-48f3-8658-325a412eb531   \n",
       "148          5.4.3  85a4042f-8972-40b9-8def-ef8d803315ab   \n",
       "149              1  4cacda81-286c-46f1-be12-7ff5626221dd   \n",
       "\n",
       "                                                  Epic Priority  \\\n",
       "0      Digitize patient records and automate tracking.     Must   \n",
       "1    Replace Trillium's current system with a new s...     Must   \n",
       "2               Implement Database for Trillium Health     Must   \n",
       "3                         Clinician Portal Development     Must   \n",
       "4                                                          Must   \n",
       "..                                                 ...      ...   \n",
       "145                                                        Must   \n",
       "146                                                        Must   \n",
       "147                                                        Must   \n",
       "148                                                        Must   \n",
       "149                                                        Must   \n",
       "\n",
       "                                            User Story Pages (Citations)  \\\n",
       "0    As a Nurse, I want to access digitized patient...             4;4;6   \n",
       "1    As a Doctor, I want to access patient data thr...             5;5;6   \n",
       "2    As a Doctor, I want to access patient clinical...               6;6   \n",
       "3    As a Doctor, I want to access and view patient...              6;13   \n",
       "4    As a Patient, I want to check in to Day Health...              10;9   \n",
       "..                                                 ...               ...   \n",
       "145  As a Clinician, I want to be able to view a pa...         11;6;6;13   \n",
       "146  As a Clinician, I want the application to comm...                15   \n",
       "147  As a Nurse, I want to update patient vitals, s...                 4   \n",
       "148  As a user, I want to add, modify, or check pat...                 5   \n",
       "149  As a Doctor, I want to easily access patient a...              4;16   \n",
       "\n",
       "     Alignment Score  Needs Review  \\\n",
       "0              0.449         False   \n",
       "1              0.179         False   \n",
       "2              0.622         False   \n",
       "3              0.222         False   \n",
       "4              0.184         False   \n",
       "..               ...           ...   \n",
       "145            0.000          True   \n",
       "146            0.000          True   \n",
       "147            0.000          True   \n",
       "148            0.000          True   \n",
       "149            0.145          True   \n",
       "\n",
       "                                       Matched Clauses  \\\n",
       "0    FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...   \n",
       "1    FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...   \n",
       "2    FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...   \n",
       "3    ISO 13485 4.2.5; FDA 21 CFR Part 11 11.10(e); ...   \n",
       "4    FDA 21 CFR Part 11 11.100; FDA 21 CFR Part 11 ...   \n",
       "..                                                 ...   \n",
       "145  ISO 13485 4.2.5; ISO 27001 A.9; IEC 62304 5.1;...   \n",
       "146  ISO 27001 A.10; ISO 27001 A.9; FDA 21 CFR Part...   \n",
       "147  FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...   \n",
       "148  FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...   \n",
       "149  FDA 21 CFR Part 11 11.10(e); FDA 21 CFR Part 1...   \n",
       "\n",
       "                  Clause Scores  \\\n",
       "0      0.63; 0.61; 0.601; 0.552   \n",
       "1      0.626; 0.594; 0.58; 0.56   \n",
       "2      0.612; 0.58; 0.573; 0.56   \n",
       "3    0.549; 0.549; 0.542; 0.512   \n",
       "4    0.613; 0.608; 0.558; 0.543   \n",
       "..                          ...   \n",
       "145  0.538; 0.527; 0.521; 0.505   \n",
       "146   0.59; 0.561; 0.554; 0.549   \n",
       "147  0.633; 0.629; 0.588; 0.541   \n",
       "148  0.667; 0.631; 0.628; 0.624   \n",
       "149    0.62; 0.595; 0.592; 0.54   \n",
       "\n",
       "                                     Expected Controls Detected Controls  \\\n",
       "0    audit_trail; data_integrity; e_signature; rbac...                     \n",
       "1    audit_trail; data_integrity; e_signature; rbac...                     \n",
       "2    audit_trail; data_integrity; e_signature; rbac...                     \n",
       "3    audit_trail; data_integrity; e_signature; rbac...                     \n",
       "4    audit_trail; data_integrity; e_signature; rbac...                     \n",
       "..                                                 ...               ...   \n",
       "145  audit_trail; data_integrity; rbac; risk_manage...                     \n",
       "146  audit_trail; data_integrity; e_signature; encr...        encryption   \n",
       "147  audit_trail; data_integrity; e_signature; rbac...                     \n",
       "148  audit_trail; data_integrity; e_signature; rbac...                     \n",
       "149  audit_trail; data_integrity; e_signature; rbac...                     \n",
       "\n",
       "                                      Missing Controls  \\\n",
       "0    audit_trail; data_integrity; e_signature; rbac...   \n",
       "1    audit_trail; data_integrity; e_signature; rbac...   \n",
       "2    audit_trail; data_integrity; e_signature; rbac...   \n",
       "3    audit_trail; data_integrity; e_signature; rbac...   \n",
       "4    audit_trail; data_integrity; e_signature; rbac...   \n",
       "..                                                 ...   \n",
       "145  audit_trail; data_integrity; rbac; risk_manage...   \n",
       "146  audit_trail; data_integrity; e_signature; rbac...   \n",
       "147  audit_trail; data_integrity; e_signature; rbac...   \n",
       "148  audit_trail; data_integrity; e_signature; rbac...   \n",
       "149  audit_trail; data_integrity; e_signature; rbac...   \n",
       "\n",
       "                              Evidence (Story + Steps)  \n",
       "0    As a Nurse, I want to access digitized patient...  \n",
       "1    As a Doctor, I want to access patient data thr...  \n",
       "2    As a Doctor, I want to access patient clinical...  \n",
       "3    As a Doctor, I want to access and view patient...  \n",
       "4    As a Patient, I want to check in to Day Health...  \n",
       "..                                                 ...  \n",
       "145  As a Clinician, I want to be able to view a pa...  \n",
       "146  As a Clinician, I want the application to comm...  \n",
       "147  As a Nurse, I want to update patient vitals, s...  \n",
       "148  As a user, I want to add, modify, or check pat...  \n",
       "149  As a Doctor, I want to easily access patient a...  \n",
       "\n",
       "[150 rows x 14 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from compliance_validator import build_compliance_report\n",
    "build_compliance_report(\n",
    "    stories_path=\"stories.json\",\n",
    "    testcases_path=\"testcases.csv\",\n",
    "    out_csv=\"compliance_evidence.csv\",\n",
    "    out_xlsx=\"compliance_evidence.xlsx\",\n",
    "    project_id=PROJECT_ID,\n",
    "    use_embeddings=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cfa2832-787e-4653-ac5e-069e7a04a51f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756969455.938828   20878 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [openpyxl]1/2\u001b[0m [openpyxl]\n",
      "\u001b[1A\u001b[2KSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1458f1a-8d9d-4683-990e-2cdd10919927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
